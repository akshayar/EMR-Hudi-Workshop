{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo2 : Schema Validation and Evolution in an S3 Datalake using Apache Hudi\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)\n",
    "3. [Schema Validation](#Schema-Validation)\n",
    "4. [Schema Evolution](#Schema-Evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is a continuation of the 1st notebook and covers the following concepts when writing Copy-On-Write tables to an S3 Datalake:\n",
    "\n",
    "- Schema Validation.\n",
    "- Schema Evolution.\n",
    "\n",
    "There are 4 cases:\n",
    "\n",
    "1. Columns are added in the middle - which is an incompatible change.\n",
    "2. Columns are dropped i.e. missing - which is a compatible change as nulls are expected for newer records.\n",
    "3. Column types are changed - is an **incompatible** change. You could try automatic casting as a resolution here.\n",
    "4. Columns are added at the end - which is a compatible change.\n",
    "\n",
    "The assumption is that all case mismatches in columns has been resolved by lower casing the fields. \n",
    "\n",
    "**This demo was run on a single node (r5.4xlarge) EMR Cluster version 5.29.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:\n",
    "\n",
    "Note that the files hudi-spark-bundle.jar and spark-avro.jar are copied into HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T22:37:40.616147Z",
     "start_time": "2020-04-06T22:37:40.597094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///hudi-spark-bundle.jar,hdfs:///spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '7G', 'spark.executor.cores': 1, 'spark.dynamicAllocation.initialExecutors': 16}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///hudi-spark-bundle.jar,hdfs:///spark-avro.jar\",\n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"7G\",\n",
    "             \"spark.executor.cores\": 1,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":16\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T22:42:05.150780Z",
     "start_time": "2020-04-06T22:42:05.123399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"example_hudi_table\",\n",
    "    \"target\": \"s3://neilawstmp2/tmp/hudi/example_hudi_table\",\n",
    "    \"primary_key\": \"id\",\n",
    "    \"sort_key\": \"sk\",\n",
    "    \"commits_to_retain\": \"2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T22:39:25.422322Z",
     "start_time": "2020-04-06T22:39:25.393617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T22:40:26.004413Z",
     "start_time": "2020-04-06T22:40:25.978772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "def get_new_json_data(start, count, increment=0):\n",
    "    data = [{\"id\": i, \"sk\": i+increment, \"txt\": chr(65 + (i % 26))} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Insert the Initial Dataset\n",
    "\n",
    "Let's generate 4M records to load into our Data Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T20:58:29.124438Z",
     "start_time": "2020-04-06T20:58:25.870857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table \"+config['table_name']).show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:04:15.780527Z",
     "start_time": "2020-04-06T20:58:31.486470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)\n",
      "\n",
      "+---+---+---+\n",
      "| id| sk|txt|\n",
      "+---+---+---+\n",
      "|  0|  0|  A|\n",
      "|  1|  1|  B|\n",
      "|  2|  2|  C|\n",
      "+---+---+---+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "df1 = create_json_df(spark, get_new_json_data(0, 4000000))\n",
    "print(df1.count())\n",
    "df1.printSchema()\n",
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And write the data to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:05:52.781505Z",
     "start_time": "2020-04-06T21:04:35.239959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:05:53.010694Z",
     "start_time": "2020-04-06T21:05:52.783325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------+\n",
      "|database|tableName         |isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "|default |example_hudi_table|false      |\n",
      "+--------+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:05:53.243422Z",
     "start_time": "2020-04-06T21:05:53.013391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE EXTERNAL TABLE `example_hudi_table`(`_hoodie_commit_time` STRING, `_hoodie_commit_seqno` STRING, `_hoodie_record_key` STRING, `_hoodie_partition_path` STRING, `_hoodie_file_name` STRING, `id` BIGINT, `sk` BIGINT, `txt` STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION 's3://neilawstmp2/tmp/hudi/example_hudi_table'\n",
      "TBLPROPERTIES (\n",
      "  'transient_lastDdlTime' = '1586207150',\n",
      "  'last_commit_time_sync' = '20200406210440'\n",
      ")\n",
      "|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table \"+config['table_name']).show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Validation\n",
    "\n",
    "Let's see how we can easily implement Schema Validation in Apache Hudi with just a few lines of code.\n",
    "\n",
    "There are 4 cases:\n",
    "\n",
    "\n",
    "1. Columns are added in the middle - which is an incompatible change.\n",
    "2. Columns are dropped i.e. missing - which is a compatible change as nulls are expected for newer records.\n",
    "3. Column types are changed - is an **incompatible** change. You could try automatic casting as a resolution here.\n",
    "4. Columns are added at the end - which is a compatible change\n",
    "\n",
    "### Adding Columns in the middle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a new column called 'new_col':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:06:16.109866Z",
     "start_time": "2020-04-06T21:06:13.864785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- new_col: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "## Generates Data\n",
    "def get_json_data(start, count, increment=0):\n",
    "    data = [{\"id\": i, \"sk\": i+increment, \"new_col\": i, \"txt\": chr(65 + (i % 26))} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "idf = create_json_df(spark, get_json_data(0, 100))\n",
    "idf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:32:55.044197Z",
     "start_time": "2020-04-06T21:32:55.017260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def validateSchema(df, table):\n",
    "    original_df=spark.sql(\"SELECT * FROM \"+table+\" LIMIT 0\")\n",
    "    columns_to_drop = ['_hoodie_commit_time', '_hoodie_commit_seqno','_hoodie_record_key','_hoodie_partition_path','_hoodie_file_name']\n",
    "    odf = original_df.drop(*columns_to_drop)\n",
    "    if (df.schema != odf.schema):\n",
    "        print (\"Schema Validation Failed : Incoming Schema is not compatible with existing Table.\")\n",
    "        if len(odf.schema) > len(df.schema):\n",
    "            print (\"Original Data Diff : \"+str(set(odf.schema)-set(df.schema)))\n",
    "        else:\n",
    "            print (\"Incoming Data Diff : \"+str(set(df.schema)-set(odf.schema)))\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:08:54.103802Z",
     "start_time": "2020-04-06T21:08:53.872878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Validation Failed : Incoming Schema is not compatible with existing Table.\n",
      "Incoming Data Diff : {StructField(new_col,LongType,true)}\n",
      "False"
     ]
    }
   ],
   "source": [
    "validateSchema(idf,config['table_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our validation function identified that a new column has been added:\n",
    "\n",
    "Let's see what would happen if we try to insert this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:07:36.734174Z",
     "start_time": "2020-04-06T21:07:07.366210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o142.save.\n",
      ": org.apache.hudi.hive.HoodieHiveSyncException: Failed in executing SQL ALTER TABLE `default.example_hudi_table` REPLACE COLUMNS(`_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `id` bigint, `new_col` bigint, `sk` bigint, `txt` string )\n",
      "\tat org.apache.hudi.hive.HoodieHiveClient.updateHiveSQL(HoodieHiveClient.java:488)\n",
      "\tat org.apache.hudi.hive.HoodieHiveClient.updateTableDefinition(HoodieHiveClient.java:264)\n",
      "\tat org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:148)\n",
      "\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:95)\n",
      "\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:67)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:236)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:169)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions :\n",
      "sk\n",
      "\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:363)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions :\n",
      "sk\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:632)\n",
      "\tat org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3590)\n",
      "\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:390)\n",
      "\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)\n",
      "\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)\n",
      "\t... 11 more\n",
      "Caused by: InvalidOperationException(message:The following columns have types incompatible with the existing columns in their respective positions :\n",
      "sk)\n",
      "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59744)\n",
      "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59730)\n",
      "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59672)\n",
      "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)\n",
      "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1693)\n",
      "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1677)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)\n",
      "\tat sun.reflect.GeneratedMethodAccessor51.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
      "\tat com.sun.proxy.$Proxy34.alter_table_with_environmentContext(Unknown Source)\n",
      "\tat sun.reflect.GeneratedMethodAccessor51.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2336)\n",
      "\tat com.sun.proxy.$Proxy34.alter_table_with_environmentContext(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:628)\n",
      "\t... 21 more\n",
      "\n",
      "\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:297)\n",
      "\tat org.apache.hudi.hive.HoodieHiveClient.updateHiveSQL(HoodieHiveClient.java:486)\n",
      "\t... 39 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 738, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o142.save.\n",
      ": org.apache.hudi.hive.HoodieHiveSyncException: Failed in executing SQL ALTER TABLE `default.example_hudi_table` REPLACE COLUMNS(`_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `id` bigint, `new_col` bigint, `sk` bigint, `txt` string )\n",
      "\tat org.apache.hudi.hive.HoodieHiveClient.updateHiveSQL(HoodieHiveClient.java:488)\n",
      "\tat org.apache.hudi.hive.HoodieHiveClient.updateTableDefinition(HoodieHiveClient.java:264)\n",
      "\tat org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:148)\n",
      "\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:95)\n",
      "\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:67)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:236)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:169)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions :\n",
      "sk\n",
      "\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:363)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions :\n",
      "sk\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:632)\n",
      "\tat org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3590)\n",
      "\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:390)\n",
      "\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)\n",
      "\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)\n",
      "\t... 11 more\n",
      "Caused by: InvalidOperationException(message:The following columns have types incompatible with the existing columns in their respective positions :\n",
      "sk)\n",
      "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59744)\n",
      "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59730)\n",
      "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59672)\n",
      "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)\n",
      "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1693)\n",
      "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1677)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)\n",
      "\tat sun.reflect.GeneratedMethodAccessor51.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
      "\tat com.sun.proxy.$Proxy34.alter_table_with_environmentContext(Unknown Source)\n",
      "\tat sun.reflect.GeneratedMethodAccessor51.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2336)\n",
      "\tat com.sun.proxy.$Proxy34.alter_table_with_environmentContext(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:628)\n",
      "\t... 21 more\n",
      "\n",
      "\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:297)\n",
      "\tat org.apache.hudi.hive.HoodieHiveClient.updateHiveSQL(HoodieHiveClient.java:486)\n",
      "\t... 39 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(idf.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Hudi's Hive Sync step would have thrown the following error message:\n",
    "\n",
    "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: \n",
    "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. \n",
    "Unable to alter table. **The following columns have types incompatible with the existing columns in their respective positions :\n",
    "sk**\n",
    "\n",
    "### Adding Columns at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate our table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:14:56.165140Z",
     "start_time": "2020-04-06T21:13:44.683276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n",
      "+--------+------------------+-----------+\n",
      "|database|tableName         |isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "|default |example_hudi_table|false      |\n",
      "+--------+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table \"+config['table_name']).show(100,False)\n",
    "spark.sql(\"show tables\").show(100,False)\n",
    "df1 = create_json_df(spark, get_new_json_data(0, 4000000))\n",
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))\n",
    "spark.sql(\"show tables\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:15:23.106666Z",
     "start_time": "2020-04-06T21:15:22.876544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)\n",
      " |-- new_col: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "## Generates Data\n",
    "def get_json_data(start, count, increment=0):\n",
    "    data = [{\"id\": i, \"sk\": i+increment, \"new_col\": i, \"txt\": chr(65 + (i % 26))} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "idf = create_json_df(spark, get_json_data(0, 100)).select(\"id\",\"sk\",\"txt\",\"new_col\")\n",
    "idf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:15:51.148764Z",
     "start_time": "2020-04-06T21:15:25.821091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(idf.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we added new columns at the end, the Hive sync step did not complain as the schema is now backwards compatible.\n",
    "\n",
    "Let's query our table to verify that the new_col has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:15:55.058673Z",
     "start_time": "2020-04-06T21:15:54.827480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)\n",
      " |-- new_col: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"select * from \"+config['table_name'])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Column Types\n",
    "\n",
    "Let's change the type of one column this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:22:42.648325Z",
     "start_time": "2020-04-06T21:21:35.188339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n",
      "+--------+------------------+-----------+\n",
      "|database|tableName         |isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "|default |example_hudi_table|false      |\n",
      "+--------+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table \"+config['table_name']).show(100,False)\n",
    "spark.sql(\"show tables\").show(100,False)\n",
    "df1 = create_json_df(spark, get_new_json_data(0, 4000000))\n",
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))\n",
    "spark.sql(\"show tables\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:24:55.635922Z",
     "start_time": "2020-04-06T21:24:55.609332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "def get_json_data(start, count, increment=0):\n",
    "    data = [{\"id\": i, \"sk\": i+increment, \"txt\": (65 + (i % 26))} for i in range(start, start + count)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:24:57.954736Z",
     "start_time": "2020-04-06T21:24:57.727175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "df1 = create_json_df(spark, get_json_data(0, 100))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:25:03.468464Z",
     "start_time": "2020-04-06T21:25:03.237182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| id| sk|txt|\n",
      "+---+---+---+\n",
      "|  0|  0| 65|\n",
      "|  1|  1| 66|\n",
      "|  2|  2| 67|\n",
      "+---+---+---+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:25:05.805073Z",
     "start_time": "2020-04-06T21:25:05.576003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Validation Failed : Incoming Schema is not compatible with existing Table.\n",
      "Incoming Data Diff : {StructField(txt,LongType,true)}\n",
      "False"
     ]
    }
   ],
   "source": [
    "validateSchema(df1,config['table_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function identifed that column types have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:25:18.849588Z",
     "start_time": "2020-04-06T21:25:07.546511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o1030.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 287.0 failed 4 times, most recent failure: Lost task 2.3 in stage 287.0 (TID 1928, ip-172-31-37-168.us-west-2.compute.internal, executor 2): org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :2\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:264)\n",
      "\tat org.apache.hudi.HoodieWriteClient.lambda$upsertRecordsInternal$507693af$1(HoodieWriteClient.java:428)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:202)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:178)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:257)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:142)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:200)\n",
      "\t... 32 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:140)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:230)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:209)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:51)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:260)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:37)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:120)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file s3://neilawstmp2/tmp/hudi/example_hudi_table/85e038d9-ac9f-49a5-9c14-89579aaa9a8c-0_2-255-1718_20200406212423.parquet\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136)\n",
      "\tat org.apache.hudi.func.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:47)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:44)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:91)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "Caused by: java.lang.UnsupportedOperationException: org.apache.parquet.avro.AvroConverters$FieldLongConverter\n",
      "\tat org.apache.parquet.io.api.PrimitiveConverter.addBinary(PrimitiveConverter.java:70)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl$2$6.writeValue(ColumnReaderImpl.java:317)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:367)\n",
      "\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:226)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:145)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :2\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:264)\n",
      "\tat org.apache.hudi.HoodieWriteClient.lambda$upsertRecordsInternal$507693af$1(HoodieWriteClient.java:428)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:202)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:178)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:257)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:142)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:200)\n",
      "\t... 32 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:140)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:230)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:209)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:51)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:260)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:37)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:120)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file s3://neilawstmp2/tmp/hudi/example_hudi_table/85e038d9-ac9f-49a5-9c14-89579aaa9a8c-0_2-255-1718_20200406212423.parquet\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136)\n",
      "\tat org.apache.hudi.func.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:47)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:44)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:91)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "Caused by: java.lang.UnsupportedOperationException: org.apache.parquet.avro.AvroConverters$FieldLongConverter\n",
      "\tat org.apache.parquet.io.api.PrimitiveConverter.addBinary(PrimitiveConverter.java:70)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl$2$6.writeValue(ColumnReaderImpl.java:317)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:367)\n",
      "\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:226)\n",
      "\t... 11 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 738, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1030.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 287.0 failed 4 times, most recent failure: Lost task 2.3 in stage 287.0 (TID 1928, ip-172-31-37-168.us-west-2.compute.internal, executor 2): org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :2\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:264)\n",
      "\tat org.apache.hudi.HoodieWriteClient.lambda$upsertRecordsInternal$507693af$1(HoodieWriteClient.java:428)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:202)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:178)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:257)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:142)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:200)\n",
      "\t... 32 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:140)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:230)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:209)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:51)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:260)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:37)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:120)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file s3://neilawstmp2/tmp/hudi/example_hudi_table/85e038d9-ac9f-49a5-9c14-89579aaa9a8c-0_2-255-1718_20200406212423.parquet\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136)\n",
      "\tat org.apache.hudi.func.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:47)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:44)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:91)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "Caused by: java.lang.UnsupportedOperationException: org.apache.parquet.avro.AvroConverters$FieldLongConverter\n",
      "\tat org.apache.parquet.io.api.PrimitiveConverter.addBinary(PrimitiveConverter.java:70)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl$2$6.writeValue(ColumnReaderImpl.java:317)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:367)\n",
      "\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:226)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:145)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :2\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:264)\n",
      "\tat org.apache.hudi.HoodieWriteClient.lambda$upsertRecordsInternal$507693af$1(HoodieWriteClient.java:428)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:202)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:178)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:257)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:142)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:200)\n",
      "\t... 32 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:140)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:230)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:209)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:51)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:260)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:37)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:120)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file s3://neilawstmp2/tmp/hudi/example_hudi_table/85e038d9-ac9f-49a5-9c14-89579aaa9a8c-0_2-255-1718_20200406212423.parquet\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136)\n",
      "\tat org.apache.hudi.func.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:47)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:44)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:91)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "Caused by: java.lang.UnsupportedOperationException: org.apache.parquet.avro.AvroConverters$FieldLongConverter\n",
      "\tat org.apache.parquet.io.api.PrimitiveConverter.addBinary(PrimitiveConverter.java:70)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl$2$6.writeValue(ColumnReaderImpl.java:317)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:367)\n",
      "\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:226)\n",
      "\t... 11 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Hudi has failed with the errors message when it tried to read the existing data as a number:\n",
    "    \n",
    "java.lang.UnsupportedOperationException: org.apache.parquet.avro.AvroConverters$FieldLongConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:27:02.088144Z",
     "start_time": "2020-04-06T21:25:50.610881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n",
      "+--------+------------------+-----------+\n",
      "|database|tableName         |isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "|default |example_hudi_table|false      |\n",
      "+--------+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table \"+config['table_name']).show(100,False)\n",
    "spark.sql(\"show tables\").show(100,False)\n",
    "df1 = create_json_df(spark, get_new_json_data(0, 4000000))\n",
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))\n",
    "spark.sql(\"show tables\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:31:30.283856Z",
     "start_time": "2020-04-06T21:31:30.260178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "def get_json_data(start, count, increment=0):\n",
    "    data = [{\"id\": i, \"sk\": i+increment} for i in range(start, start + count)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:31:33.092036Z",
     "start_time": "2020-04-06T21:31:32.862072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "df1 = create_json_df(spark, get_json_data(0, 100))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:31:34.918720Z",
     "start_time": "2020-04-06T21:31:34.689842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id| sk|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "+---+---+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:32:35.663115Z",
     "start_time": "2020-04-06T21:32:35.434258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Validation Failed : Incoming Schema is not compatible with existing Table.\n",
      "Original Data Diff : {StructField(txt,StringType,true)}\n",
      "False"
     ]
    }
   ],
   "source": [
    "validateSchema(df1,config['table_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:27:45.667588Z",
     "start_time": "2020-04-06T21:27:34.372946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o1144.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 320.0 failed 4 times, most recent failure: Lost task 1.3 in stage 320.0 (TID 2149, ip-172-31-37-168.us-west-2.compute.internal, executor 3): org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :1\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:264)\n",
      "\tat org.apache.hudi.HoodieWriteClient.lambda$upsertRecordsInternal$507693af$1(HoodieWriteClient.java:428)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:202)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:178)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:257)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:142)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:200)\n",
      "\t... 32 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:140)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:230)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:209)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:51)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:260)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:37)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:120)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'txt' not found\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:225)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:130)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)\n",
      "\tat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\n",
      "\tat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\n",
      "\tat org.apache.hudi.func.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:47)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:44)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:91)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:145)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :1\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:264)\n",
      "\tat org.apache.hudi.HoodieWriteClient.lambda$upsertRecordsInternal$507693af$1(HoodieWriteClient.java:428)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:202)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:178)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:257)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:142)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:200)\n",
      "\t... 32 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:140)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:230)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:209)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:51)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:260)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:37)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:120)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'txt' not found\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:225)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:130)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)\n",
      "\tat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\n",
      "\tat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\n",
      "\tat org.apache.hudi.func.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:47)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:44)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:91)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 738, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1144.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 320.0 failed 4 times, most recent failure: Lost task 1.3 in stage 320.0 (TID 2149, ip-172-31-37-168.us-west-2.compute.internal, executor 3): org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :1\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:264)\n",
      "\tat org.apache.hudi.HoodieWriteClient.lambda$upsertRecordsInternal$507693af$1(HoodieWriteClient.java:428)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:202)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:178)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:257)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:142)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:200)\n",
      "\t... 32 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:140)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:230)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:209)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:51)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:260)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:37)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:120)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'txt' not found\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:225)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:130)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)\n",
      "\tat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\n",
      "\tat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\n",
      "\tat org.apache.hudi.func.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:47)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:44)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:91)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:145)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :1\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:264)\n",
      "\tat org.apache.hudi.HoodieWriteClient.lambda$upsertRecordsInternal$507693af$1(HoodieWriteClient.java:428)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:202)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:178)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:257)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:142)\n",
      "\tat org.apache.hudi.table.HoodieCopyOnWriteTable.handleUpdateInternal(HoodieCopyOnWriteTable.java:200)\n",
      "\t... 32 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:140)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:230)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:209)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:51)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:260)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:37)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:120)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'txt' not found\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:225)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:130)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)\n",
      "\tat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\n",
      "\tat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\n",
      "\tat org.apache.hudi.func.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:47)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:44)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:91)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hudi threw an error message:\n",
    "    \n",
    "Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'txt' not found\n",
    "\n",
    "What if we added a null column called 'txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:28:54.801115Z",
     "start_time": "2020-04-06T21:28:54.569469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df1 = create_json_df(spark, get_json_data(0, 100)).withColumn(\"txt\",lit(None).cast(StringType()))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:29:23.484347Z",
     "start_time": "2020-04-06T21:28:58.123825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:29:38.150528Z",
     "start_time": "2020-04-06T21:29:28.879627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "|   id|   sk| txt|\n",
      "+-----+-----+----+\n",
      "|    0|    0|null|\n",
      "|10000|10000|   Q|\n",
      "+-----+-----+----+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id,sk, txt from \"+config['table_name']+' where id in (0,10000)').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Evolution\n",
    "\n",
    "To summarize: we tested the 4 cases:\n",
    "\n",
    "1. Columns are added at the end - which is a compatible change\n",
    "2. Columns are added in the middle - which is an incompatible change but could be resolved by rearranging the input columns.\n",
    "3. Columns are dropped - which is an incompatible change but could be resolved the adding a null column. \n",
    "4. Column types are changed - which is an incompatible change and can potentially be resolved by casting the changed column to the right type.\n",
    "\n",
    "Let's add columns at the end to ensure our schema remains compatible with the original schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T22:41:51.089674Z",
     "start_time": "2020-04-06T22:41:51.063770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "def get_json_data(start, count, increment=0):\n",
    "    data = [{\"id\": i, \"sk\": i+increment, \"new_col\": i, \"txt\": chr(65 + (i % 26))} for i in range(start, start + count)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T22:44:15.288933Z",
     "start_time": "2020-04-06T22:42:43.663845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n",
      "+--------+------------------+-----------+\n",
      "|database|tableName         |isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "|default |example_hudi_table|false      |\n",
      "+--------+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists \"+config['table_name']).show(100,False)\n",
    "spark.sql(\"show tables\").show(100,False)\n",
    "df1 = create_json_df(spark, get_new_json_data(0, 4000000))\n",
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))\n",
    "spark.sql(\"show tables\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T23:23:48.314211Z",
     "start_time": "2020-04-06T23:23:48.288458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evolveSchema(df, table,strict=False,forcecast=True):\n",
    "    original_df=spark.sql(\"SELECT * FROM \"+table+\" LIMIT 0\")\n",
    "    columns_to_drop = ['_hoodie_commit_time', '_hoodie_commit_seqno','_hoodie_record_key','_hoodie_partition_path','_hoodie_file_name']\n",
    "    odf = original_df.drop(*columns_to_drop)\n",
    "    if (df.schema != odf.schema):\n",
    "        if (strict):\n",
    "            print (\"Strict Schema Validation Failed : Incoming Schema is not compatible with existing Table.\")\n",
    "            if len(odf.schema) > len(df.schema):\n",
    "                print (\"Original Data Diff : \"+str(set(odf.schema)-set(df.schema)))\n",
    "            else:\n",
    "                print (\"Incoming Data Diff : \"+str(set(df.schema)-set(odf.schema)))\n",
    "            return (False,df)\n",
    "        else:\n",
    "            new_cols=[s for s in list(set([s.name for s in df.schema.fields])-set([s.name for s in odf.schema.fields]))]\n",
    "            if forcecast:\n",
    "                ## force cast columns \n",
    "                existing_cols=[f'cast({s.name} as {s.dataType.typeName()}) {s.name}' for s in odf.schema.fields]\n",
    "                #print  (existing_cols)\n",
    "                #print  (new_cols)\n",
    "                new_df = df.selectExpr(existing_cols+new_cols)\n",
    "            else:    \n",
    "                existing_cols=[s.name for s in odf.schema.fields]\n",
    "                new_df = df.select(existing_cols+new_cols)\n",
    "            ## re-arrange the columns to ensure schema compatibility\n",
    "            \n",
    "            return (True, new_df)\n",
    "    return (True,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform a type change as well as add a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T23:26:09.792409Z",
     "start_time": "2020-04-06T23:26:09.765578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "def get_json_data(start, count, increment=0):\n",
    "    data = [{\"id\": i, \"sk\": i+increment, 'new':i, \"txt\": (65 + (i % 26))} for i in range(start, start + count)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T23:26:12.326642Z",
     "start_time": "2020-04-06T23:26:12.097917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- new: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "idf = create_json_df(spark, get_json_data(0, 100))\n",
    "idf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T23:26:20.308872Z",
     "start_time": "2020-04-06T23:26:20.080088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)\n",
      " |-- new: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "(schemaValidationResult,evolvedDF)=evolveSchema(idf,config['table_name'],False)\n",
    "evolvedDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now observe schema evolution has rearranged the columns to what is expected by our Hudi table and has fixed the datatypes as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T23:29:35.312118Z",
     "start_time": "2020-04-06T23:29:07.951428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(evolvedDF.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T23:32:01.300993Z",
     "start_time": "2020-04-06T23:32:01.071668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)\n",
      " |-- new: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"select * from \"+config['table_name'])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
